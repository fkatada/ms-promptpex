---
title: Test Evaluation
sidebar:
    order: 21.5
---
Given a set of tests, the next step is to evaluate the prompt under test and a particular model under test against those tests.  



```mermaid
graph TD
    PUT[("Prompt Under Test (PUT)")]
    MUT[("Model Under Test (PUT)")]
    PPT{{"PromptPex Tests (PPT)"}}
    RPMT{{"Run Tests with PUT and MUT"}}   
    TO["Test Output (TO) for PUT, MUT, PPT"]
    ETM[("Eval Test Metrics (ETM)")]
    ETNC[("Eval Test Non-Compliance (ETNC)")] 

    PUT ==> RPMT
    MUT ==> RPMT
    PPT ==> RPMT
    RPMT ==> TO
    TO ==> ETM
    TO ==> ETNC

```

### Model Under Test

Test results will depend on both the PUT and the **Model Under Test** (MUT).  PromptPex allows the user to specify any number of MUTs to allow the user to understand how their prompt performs across different models.  Running the tests for a given PUT and MUT will produce a set of outputs for each test.  This output can then be evaluated using different metrics to understand how well the prompt performs.  

### Evaluation Metrics

PromptPex supports different metrics to evaluate the performance of the PUT, MUT, and tests.  PromptPex has a built-in metric, **Test for Non-Compliance** (TNC), which checks if the output of the prompt meets the constraints specified in the PUT.  This is done by checking the output against the input specification and output rules of the PUT.

PromptPex also supports user-defined metrics.  These metrics are defined in a prompty file with a naming convention `METRICNAME.metric.prompty`.  Any files following this naming convention and located in the same directory as the PUT will be automatically detected and used as metrics.
 
The prompty file that defines the metric should contain a `system` section that describes the metric and how it should be evaluated.  The available input parameters to the metric are:

- **output**: The output of the prompt under test.
- **prompt**: The prompt under test.
- **rules**: The rules that the output must comply with.
- **input**: The input to the prompt under test.

Here is an example of a user-defined metric that uses the rules to determine if the output complies with the rules:
```markdown wrap title="use_rules.metric.prompty"
system:
Your task is to very carefully and thoroughly evaluate the given output generated by a chatbot in <CHATBOT_OUTPUT> to find out if it comply with all the rules provided to you in <RULES>.

Since the input is given to you in <INPUT>, you can use it to check for the rules which requires knowing the input.

### Here are the guidelines to follow for your evaluation process:

1. **Direct Compliance Only**: Your evaluation should be based solely on direct and explicit compliance with the rules provided. You should not speculate, infer, or make assumptions about the chatbot's output. Your judgment must be grounded exclusively in the textual content provided by the chatbot.

2. **Decision as Compliance Score**: You are required to generate a compliance score based on your evaluation:
   - Return 100 if <CHATBOT_OUTPUT> complies with all the rules
   - Return 0 if it does not comply with any of the rules.
   - Return a score between 0 and 100 if <CHATBOT_OUTPUT> partially complies with the rules
   - In the case of partial compliance, you should based on the importance of the rules and the severity of the violations, assign a score between 0 and 100. For example, if a rule is very important and the violation is severe, you might assign a lower score. Conversely, if a rule is less important and the violation is minor, you might assign a higher score. 

3. **Compliance Statement**: Carefully examine the output and determine why the output does not comply with the rules, think of reasons why the output complies or does not compiles with the rules, citing specific elements of the output.

4. **Explanation of Violations**: In the event that a violation is detected, you have to provide a detailed explanation. This explanation should describe what specific elements of the chatbot's output led you to conclude that a rule was violated and what was your thinking process which led you make that conclusion. Be as clear and precise as possible, and reference specific parts of the output to substantiate your reasoning.

5. **Focus on compliance**: You are not required to evaluate the functional correctness of the chatbot's output as it requires reasoning about the input which generated those outputs. Your evaluation should focus on whether the output complies with the rules, if it requires knowing the input, use the input given to you.

6. **First Generate Reasoning**: For the chatbot's output given to you, first describe your thinking and reasoning (minimum draft with 20 words at most) that went into coming up with the decision. Answer in English.

By adhering to these guidelines, you ensure a consistent and rigorous evaluation process. Be very rational and do not make up information. Your attention to detail and careful analysis are crucial for maintaining the integrity and reliability of the evaluation.

<RULES> 
{{rules}}
</RULES>

user:
<CHATBOT_OUTPUT>
{{output}}
</CHATBOT_OUTPUT>

```

## Input Specification

The [first transformation](https://github.com/microsoft/promptpex/blob/dev/src/prompts/generate_intent.prompty) takes the prompt under test and extracts the **input specification (IS)**. 
The input specification is a description of the input to the prompt. 
In this case, the input consists of a sentence and a word from that sentence.

```mermaid
graph LR
    PUT(["Prompt Under Test (PUT)"])
    IS["Input Specification (IS)"]

    PUT --> IS
```

```text wrap title="Input Specification"
The input consists of a sentence combined with a specific word from that sentence.
The sentence must contain natural language text.
The word must be a single word from the provided sentence.
```

## Output Rules

The [second transformation](https://github.com/microsoft/promptpex/blob/dev/src/prompts/generate_output_rules.prompty) takes the prompt under test and extracts the **output rules (OR)**.
The output rules are a description of the output of the prompt.
In this case, the output consists of a part of speech tag for the word.

```mermaid
graph LR
    PUT(["Prompt Under Test (PUT)"])
    OR["Output Rules (OR)"]

    PUT --> OR
```

```text wrap title="Output Rules"
The output must return only the part of speech tag without any additional text or formatting.
If the given word can be identified with one of the listed part of speech tags, the output must include only the specific tag for that word from the provided alphabetical list.
If the given word cannot be tagged with any of the listed part of speech tags, the output should be the word "Unknown".
If tagging the given word is not possible for any reason, the output should be the word "CantAnswer".
```

## Inverse Output Rules

The [third transformation](https://github.com/microsoft/promptpex/blob/dev/src/prompts/generate_inverse_rules.prompty) takes the output rules and generates the **inverse output rules (IOR)**.
The inverse output rules are a description of the output of the prompt that is the opposite of the output rules.
In this case, the inverse output rules are a description of the output of the prompt that is the opposite of the output rules.

```mermaid
graph LR
    PUT(["Prompt Under Test (PUT)"])
    OR["Output Rules (OR)"]
    IOR["Inverse Output Rules (IOR)"]

    PUT --> OR
    OR ==> IOR
```

```text wrap title="Inverse Output Rules"
The output must not return any additional text or formatting.
The output must not include any of the listed part of speech tags.
The output must not include the word "Unknown".
The output must not include the word "CantAnswer".
```

## Tests generated from the rules:

From the input specification, output rules, inverse output rules, PromptPex uses a [LLM prompt](https://github.com/microsoft/promptpex/blob/dev/src/prompts/generate_tests.prompty) to generate a set of tests.
```mermaid
graph TD
    PUT(["Prompt Under Test (PUT)"])
    IS["Input Specification (IS)"]
    OR["Output Rules (OR)"]
    IOR["Inverse Output Rules (IOR)"]
    PPT["PromptPex Tests (PPT)"]

    PUT --> IS
    PUT --> OR
    OR --> IOR
    IS ==> PPT
    OR ==> PPT
    PUT ==> PPT
    IOR ==> PPT
```        

```text wrap
sentence: 'An aura of mystery surrounded them.', word: 'aura'
sentence: 'The researchers documented carefully.', word: 'carefully'
# Note this tests the Unknown corner case
sentence: 'This is such a unique perspective.', word: 'such'
```

At this point, we have a set of inputs and predicted outputs that we can use in a variety of ways.